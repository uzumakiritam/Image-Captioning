# Multimodal Image Captioning and Caption Classification

## Overview
This project implements a **custom image captioning system** using a **Vision Transformer (ViT) + GPT-2** encoder-decoder framework and evaluates its robustness under **image occlusion**.  
Additionally, a **BERT-based caption classifier** is trained to identify caption sources and measure performance under perturbation.  
All experiments were conducted on a Google Colab T4 GPU.

---

## Part A: Custom Encoder-Decoder Model

### Methodology
The image captioning model integrates:
- **Encoder:** `ViT-small-patch16-224` pretrained model, outputting 768-dim [CLS] embeddings.
- **Feature Projection Layer:** Linear + ReLU, mapping ViT output to GPT-2’s 1024-dim space.
- **Decoder:** Modified GPT-2 that accepts projected image embeddings as context input.
- **Cross-Attention:** Multi-head attention enabling the decoder to attend to image patch embeddings.
- **Freezing Strategy:** First 6 ViT layers frozen to prevent overfitting.

### Training Setup
| Parameter | Value |
|------------|--------|
| Batch Size | 16 |
| Learning Rate | 5e-5 |
| Epochs | 5 |
| Optimizer | AdamW |
| Loss | CrossEntropyLoss |
| Gradient Clipping | 1.0 |

**Memory Optimizations:**
- Mixed Precision Training (FP16)
- Gradient Accumulation (4 steps)
- Dynamic Padding/Collation

Checkpointing and averaged loss recorded after each epoch.

---

## Results

### Test Set Performance
| Model | BLEU | ROUGE-L | METEOR |
|--------|-------|----------|----------|
| SmolVLM | 0.0275 | 0.2244 | 0.1747 |
| Custom Model | **0.0444** | **0.2836** | **0.2082** |

**Improvement Drivers:**
- Domain-specific fine-tuning (+61% BLEU)
- Reduced modality mismatch
- Progressive unfreezing and cosine LR scheduler

---

## Part B: Occlusion Robustness Study

Performance under increasing image occlusion (0–80%):

| Metric | Custom Model Decay | SmolVLM Decay |
|---------|--------------------|----------------|
| BLEU | ↓15.4% | ↓70.1% |
| ROUGE-L | ↓4.2% | ↓13.0% |
| METEOR | ↓7.5% | ↓15.7% |

**Findings:**
- Custom model is **3–5× more stable** under visual degradation.
- BLEU is the most sensitive metric.
- Custom model generalizes well under partial visual loss.

---

## Overall Comparison

| Metric | Custom Avg | SmolVLM Avg | Relative Gain |
|---------|-------------|--------------|----------------|
| BLEU | 0.0551 | 0.0081 | +580% |
| ROUGE-L | 0.2433 | 0.1648 | +47.6% |
| METEOR | 0.2524 | 0.1385 | +82.2% |

**Conclusion:**  
The custom model achieves consistent improvements in accuracy and robustness, making it a suitable choice for real-world deployment where input quality may vary.

---

## Part C: Caption Classification

### Architecture
- **Base Model:** `bert-base-uncased`
- **Layers Frozen:** All BERT encoder layers
- **Classification Head:**  
  - 1 hidden layer (ReLU)  
  - Dropout 0.3  
  - Output Softmax (binary classification)

### Dataset
Captions generated by multiple captioning models under perturbations (0%, 10%, 50%, 80%).  
Each caption labeled according to its generating model.

---

## Classification Results

| Dataset | Accuracy | Precision | Recall | F1 Score |
|----------|-----------|------------|----------|-----------|
| Validation | 0.9785 | 0.9794 | 0.9785 | 0.9785 |
| Test | 0.9839 | 0.9844 | 0.9839 | 0.9839 |

**Perturbation Results:**
| Model | Accuracy | F1 Score |
|--------|-----------|-----------|
| Custom (0–80%) | 1.0000 | 1.0000 |
| SmolVLM (0–80%) | 0.9677 | 0.4918 |

**Cross-Perturbation (Train vs Test 0–80%)**  
Accuracy: 0.9839 | F1: 0.9839  
→ Excellent generalization across perturbation levels.

---

## Key Insights
- **Custom Model:** Robust, reliable, and stable under partial visual input; suitable for real-world use.
- **SmolVLM:** Struggles with occlusion; needs stronger cross-modal feature fusion.
- **BERT Classifier:** Highly accurate in identifying model-generated captions even under heavy noise.

---
